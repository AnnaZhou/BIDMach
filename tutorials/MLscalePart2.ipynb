{
 "metadata": {
  "name": "",
  "signature": "sha256:53478800e09f270ba26a9baa6234bc6470382af216f9c08d3b678f72d8c2559f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Machine Learning at Scale, Part II"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First lets initialize BIDMach again."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import BIDMat.{CMat,CSMat,DMat,Dict,IDict,Image,FMat,FND,GMat,GIMat,GSMat,HMat,IMat,Mat,SMat,SBMat,SDMat}\n",
      "import BIDMat.MatFunctions._\n",
      "import BIDMat.SciFunctions._\n",
      "import BIDMat.Solvers._\n",
      "import BIDMat.Plotting._\n",
      "import BIDMach.Learner\n",
      "import BIDMach.models.{FM,GLM,KMeans,KMeansw,LDA,LDAgibbs,Model,NMF,SFA}\n",
      "import BIDMach.datasources.{DataSource,MatDS,FilesDS,SFilesDS}\n",
      "import BIDMach.mixins.{CosineSim,Perplexity,Top,L1Regularizer,L2Regularizer}\n",
      "import BIDMach.updaters.{ADAGrad,Batch,BatchNorm,IncMult,IncNorm,Telescoping}\n",
      "import BIDMach.causal.{IPTW}\n",
      "\n",
      "Mat.checkMKL\n",
      "Mat.checkCUDA\n",
      "if (Mat.hasCUDA > 0) GPUmem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the GPU memory above. It should be close to 1 again (0.98 or greater). If not, check if you have any \"zombie\" java processes from the linux shell with <code>ps -ef | grep java</code>. You will normally have two processes, one for the main IScala shell and one for this notebook. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sourcing Data from Disk"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training models with data that fits in memory is very limiting. But minibatch learners can easily work with data directly from disk. They access data sequentially (which avoids seeking and maximizes disk throughput) and converge in very few passes. We'll look again at the MNIST data set, this time the full version with 8 million images (about 10 GB, and 120x larger than the small MNIST dataset). The dataset has been partition into groups of 100k images (e.g. using the unix split command, or in a cluster) and saved in compressed lz4 files. \n",
      "\n",
      "We're also going to build up a learner in stages. First we're going to define a mixture class to hold all the options."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class xopts extends Learner.Options with FilesDS.Opts with GLM.Opts with ADAGrad.Opts; \n",
      "val mnopts = new xopts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next come the options for the data source. There are quite a few of these, but they only need to be set once and apart from the minibatch size, dont need to be tuned. \n",
      "\n",
      "The following options specify various useful traits of the data source. Many of these are default values and dont actually need to be set, but its useful to know what they do. Most options are explained below. \n",
      "\n",
      "The <code>fnames</code> option specifies a **list of functions** that return a filename given an integer. Its a list so you can specify a single datasource that combines data from different files, e.g. data files and target files for regression. The method <code>FilesDS.simpleEnum()</code> takes a format string argument and returns such a function that uses the format string and an integer argument to construct the filename to retrieve. So this source builds data blocks from part00.smat.lz4, part01.smat.lz4,... It stacks the rows of \"cats\" matrices on top of the rows of the \"part\" (data) matrices. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val mdir = \"../data/MNIST8M/parts/\"\n",
      "mnopts.fnames = List(FilesDS.simpleEnum(mdir+\"data%02d.fmat.lz4\", 1, 0),  // File name templates, %02d is replaced by a number\n",
      "                     FilesDS.simpleEnum(mdir+\"cats%02d.fmat.lz4\", 1, 0));\n",
      "mnopts.nstart = 0;                 // Starting file number\n",
      "mnopts.nend = 70;                  // Ending file number\n",
      "mnopts.order = 0;                  // (0) sample order, 0=linear, 1=random\n",
      "mnopts.lookahead = 2;              // (2) number of prefetch threads\n",
      "mnopts.featType = 1;               // (1) feature type, 0=binary, 1=linear\n",
      "mnopts.addConstFeat = true         // add a constant feature (effectively adds a $\\beta_0$ term to $X\\beta$)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next options define the type of GLM model (linear, logistic etc) through the links value.\n",
      "They also specify through the targets matrix, how to extract target values from the input matrix.\n",
      "Finally the mask is a vector that specifies which data values can be used for training: i.e. it should exclude the target values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mnopts.links = GLM.maxp*iones(10,1)\n",
      "mnopts.autoReset = false            // Dont reset the GPU after the training run, so we can use a GPU model for prediction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we'll create a custom datasource. We need a bit of runtime configuration to ensure that the datasource runs reliably. Because it prefetches files with separate threads, we need to make sure that enough threads are available or it will stall. The <code>threadPool(4)</code> call takes care of this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val ds = {\n",
      "  implicit val ec = threadPool(4)   // make sure there are enough threads (more than the lookahead count)\n",
      "  new FilesDS(mnopts)              // the datasource\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we define the main learner class, which is built up from various \"plug-and-play\" learning modules."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val nn = new Learner(                // make a learner instance\n",
      "    ds,                              // datasource\n",
      "    new GLM(mnopts),                 // the model (a GLM model)\n",
      "    null,                            // list of mixins or regularizers\n",
      "    new ADAGrad(mnopts),             // the optimization class to use\n",
      "    mnopts)                          // pass the options to the learner as well\n",
      "nn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tuning Options"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following options are the important ones for tuning. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mnopts.batchSize = 1000\n",
      "mnopts.npasses = 2\n",
      "mnopts.lrate = 0.001"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You invoke the learner the same way as before. You can change the options above after each run to optimize performance. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn.train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note the relatively low gflops throughput and high I/O throughput for this dataset. This problem is clearly I/O bound. This is often true for regression problems, especially those with few targets (there were 10 here vs. 104 for RCV1). To study computing limits, we'll next do a compute-intensive problem: K-means clustering. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Shutdown this document before continuing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Go back to your master notebook page, and click on **shutdown** next to this page. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}