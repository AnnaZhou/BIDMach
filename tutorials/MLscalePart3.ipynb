{
 "metadata": {
  "name": "",
  "signature": "sha256:9de8741959f387921ffa460d3d765770c690044a138201336d1766560f77756f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Machine Learning at Scale, Part III"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import BIDMat.{CMat,CSMat,DMat,Dict,IDict,Image,FMat,FND,GMat,GIMat,GSMat,HMat,IMat,Mat,SMat,SBMat,SDMat}\n",
      "import BIDMat.MatFunctions._\n",
      "import BIDMat.SciFunctions._\n",
      "import BIDMat.Solvers._\n",
      "import BIDMat.Plotting._\n",
      "import BIDMach.Learner\n",
      "import BIDMach.models.{FM,GLM,KMeans,KMeansw,LDA,LDAgibbs,Model,NMF,SFA}\n",
      "import BIDMach.datasources.{DataSource,MatDS,FilesDS,SFilesDS}\n",
      "import BIDMach.mixins.{CosineSim,Perplexity,Top,L1Regularizer,L2Regularizer}\n",
      "import BIDMach.updaters.{ADAGrad,Batch,BatchNorm,IncMult,IncNorm,Telescoping}\n",
      "import BIDMach.causal.{IPTW}\n",
      "\n",
      "Mat.checkMKL\n",
      "Mat.checkCUDA\n",
      "if (Mat.hasCUDA > 0) GPUmem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "KMeans clustering at scale"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Training models with data that fits in memory is very limiting. But minibatch learners can easily work with data directly from disk. They access data sequentially (which avoids seeking and maximizes disk throughput) and converge in very few passes. We'll look again at the MNIST data set, this time the full version with 8 million images (about 17 GB). The dataset has been partition into groups of 100k images (using the unix split command) and saved in compressed lz4 files. \n",
      "\n",
      "If you dont yet have the \"alls\" files, you can build them from the cats and part files with the cell below. Just uncomment the for loop. As you can see, alls is built from the data matrix and a scaled-up copy of the category matrix. The exaggerated category feature forces each cluster to be pure - of one category. It also creates a category feature for the cluster which can be used to retrieve its dominant category for labeling. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val mdir = \"../data/MNIST8M/parts/\"\n",
      "/* for (i <- 0 to 80) {\n",
      "    val a = loadFMat(mdir+\"part%02d.fmat.lz4\" format i)\n",
      "    val c = loadFMat(mdir+\"cats%02d.fmat.lz4\" format i)\n",
      "    saveFMat(mdir+\"data%02d.fmat.lz4\" format i, a)\n",
      "    val alls = (c * 10000f) on a\n",
      "    saveFMat(mdir+\"alls%02d.fmat.lz4\" format i, alls)\n",
      "    print(\".\")\n",
      "} */"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we define a class xopts which contains all the options for the learner we will use. BIDMach has a modular design with pluggable learning pieces. This class holds all the options for those pieces. Learner is the main learning class, FilesDS is the data source, KMeans is the model and Batch is the updater (the code that gets run to update the model) for KMeans. \n",
      "\n",
      "Then we make an instance of that class called mnopts. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class xopts extends Learner.Options with FilesDS.Opts with KMeans.Opts with Batch.Opts; \n",
      "val mnopts = new xopts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next come the options for the data source. There are quite a few of these, but they only need to be set once and apart from the minibatch size, dont need to be tuned. \n",
      "\n",
      "The following options specify various useful traits of the data source. Many of these are default values and dont actually need to be set, but its useful to know what they do."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mnopts.fnames = List(FilesDS.simpleEnum(mdir+\"alls%02d.fmat.lz4\", 1, 0));  // File name templates, %02d is replaced by a number\n",
      "mnopts.nstart = 0;                 // Starting file number\n",
      "mnopts.nend = 80;                  // Ending file number\n",
      "mnopts.order = 0;                  // (0) sample order, 0=linear, 1=random\n",
      "mnopts.lookahead = 2;              // (2) number of prefetch threads\n",
      "mnopts.featType = 1;               // (1) feature type, 0=binary, 1=linear"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we define the number of kmeans clusters.\n",
      "\n",
      "Autoreset is an option that tells the Learner not to reset GPU memory after training. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mnopts.autoReset = false            // Dont reset the GPU after the training run, so we can use a GPU model for prediction\n",
      "mnopts.dim = 300                    // Number of kmeans clusters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we'll create a custom datasource. We need a bit of runtime configuration to ensure that the datasource runs reliably. Because it prefetches files with separate threads, we need to make sure that enough threads are available or it will stall. The <code>threadPool(4)</code> call takes care of this."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val ds = {\n",
      "  implicit val ec = threadPool(4)   // make sure there are enough threads (more than the lookahead count)\n",
      "  new FilesDS(mnopts)              // the datasource\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we define the main learner class, which is built up from various \"plug-and-play\" learning modules."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val nn = new Learner(                // make a learner instance\n",
      "    ds,                              // datasource\n",
      "    new KMeans(mnopts),              // the model (a KMeans model)\n",
      "    null,                            // list of mixins or regularizers\n",
      "    new Batch(mnopts),               // the optimization class to use\n",
      "    mnopts)                          // pass the options to the learner as well\n",
      "nn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tuning Options"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following options are the important ones for tuning. For KMeans, batchSize has no effect on accracy since the algorithm uses all the data instances to perform an update. So you're free to tune it for best speed. Generally larger is better, as long as you dont use too much GPU ram. \n",
      "\n",
      "npasses is the number of passes over the dataset. Larger is typically better, but the model may overfit at some point. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mnopts.batchSize = 50000\n",
      "mnopts.npasses = 6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You invoke the learner the same way as before. You can change the options above after each run to optimize performance. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn.train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now lets extract the model as a Floating-point matrix. We included the category features for clustering to make sure that each cluster is a subset of images for one digit. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val model=FMat(nn.modelmat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we build a 30 x 10 array of images to view the first 300 cluster centers as images."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val nx = 30\n",
      "val ny = 10\n",
      "val im = zeros(28,28)\n",
      "val allim = zeros(28*nx,28*ny)\n",
      "for (i<-0 until nx) {\n",
      "    for (j<-0 until ny) {\n",
      "        val slice = model(i+nx*j,10->794)\n",
      "        im(?) = slice(?)\n",
      "        allim((28*i)->(28*(i+1)), (28*j)->(28*(j+1))) = im\n",
      "    }\n",
      "}\n",
      "Image.show(allim kron ones(2,2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll predict using the closest cluster (or 1-NN if you like). The classify function below takes a block of data (which includes the labels in rows 0->10), and predicts using the other features. It then stacks the predicted and actual categories in a <code>2*k</code> matrix, where k is the number of samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val datamodel = model(?, 10->794)\n",
      "val catmodel = model(?, 0->10)\n",
      "val (vcat, icat) = maxi2(catmodel,2)\n",
      "val mdot = (datamodel \u2219\u2192 datamodel)\n",
      "\n",
      "def classify(a:FMat):IMat = {\n",
      "    val cdata = a(0->10, ?);\n",
      "    val (vm, im) = maxi2(cdata);\n",
      "    val ddata = a(10->794, ?);\n",
      "    val dists = -2 *(datamodel * ddata) + (ddata \u2219 ddata) + mdot;\n",
      "    val (vdist, idist) = mini2(dists);\n",
      "    icat(idist) on im\n",
      "}\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The cmatch function takes the actual and predicted categories and constructs a 10x10 **confusion matrix** from them. The confusion matrix element c(i,j) is the count of inputs that were predicted to be in category i, but are actually in category j. Its basically just a call to the accum function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cmatch(crows:IMat):DMat = {\n",
      "    accum(crows.t, 1.0, 10, 10)\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To evaluate, we'll run the classification on the remainder of the data source (files 71 to 80 which we didnt read yet). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mnopts.nstart=80\n",
      "mnopts.nend=81\n",
      "ds.reset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This code draws minibatches from the datasource, computes predictions from them, and then adds the corresponding counts to the confusion matrix <code>acc</code>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "var k = 0\n",
      "val acc = dzeros(10,10)\n",
      "while (ds.hasNext) {\n",
      "    val mats=ds.next\n",
      "    val f=FMat(mats(0))\n",
      "    acc ~ acc + cmatch(classify(f))\n",
      "    k += 1\n",
      "    print(\".\")\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have the confusion counts, we can normalize the matrix of counts to produce a matrix sacc which is the fraction of samples with actual label j that are classified as i. \n",
      "\n",
      "Its common to show this matrix as a 2D gray-scale or false-color plot with white as 1.00 and black as 0.0. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val sacc = FMat(acc/sum(acc))\n",
      "Image.show((sacc * 250f) \u2297 ones(64,64))\n",
      "sacc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Its useful to isolate the correct classification rate by digit, which is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val dacc = getdiag(sacc).t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can take the mean of the diagonal accuracies to get an overall accuracy for this model. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean(dacc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run the experiment again with a larger number of clusters (3000, then 30000). You should reduce the batchSize option to 20000 to avoid memory problems.\n",
      "\n",
      "Include the training time output by the call to <code>nn.train</code> but not the evaluation time (the evaluation code above is not using the GPU). Rerun and fill out the table below: \n",
      "\n",
      "<table>\n",
      "<tr>\n",
      "<th>KMeans Clusters</th>\n",
      "<th>Training time</th>\n",
      "<th>Avg. gflops</th>\n",
      "<th>Accuracy</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>300</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>3000</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>30000</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "</tr>\n",
      "</table>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}