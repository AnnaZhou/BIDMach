{
 "metadata": {
  "name": "",
  "signature": "sha256:e2a90ff34b1605a7343ec1a0612bf53b05805bef282bff312bbeb2ec65ac2a4a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Machine Learning at Scale, Part IV"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First lets initialize BIDMach again."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import BIDMat.{CMat,CSMat,DMat,Dict,IDict,Image,FMat,FND,GMat,GIMat,GSMat,HMat,IMat,Mat,SMat,SBMat,SDMat}\n",
      "import BIDMat.MatFunctions._\n",
      "import BIDMat.SciFunctions._\n",
      "import BIDMat.Solvers._\n",
      "import BIDMat.Plotting._\n",
      "import BIDMach.Learner\n",
      "import BIDMach.models.{FM,GLM,KMeans,KMeansw,LDA,LDAgibbs,Model,NMF,SFA}\n",
      "import BIDMach.datasources.{DataSource,MatDS,FilesDS,SFilesDS}\n",
      "import BIDMach.mixins.{CosineSim,Perplexity,Top,L1Regularizer,L2Regularizer}\n",
      "import BIDMach.updaters.{ADAGrad,Batch,BatchNorm,IncMult,IncNorm,Telescoping}\n",
      "import BIDMach.causal.{IPTW}\n",
      "\n",
      "Mat.checkMKL\n",
      "Mat.checkCUDA\n",
      "if (Mat.hasCUDA > 0) GPUmem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the GPU memory again, and make sure you dont have any dangling processes."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Large-scale Topic Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This time, you will build up the datasource and learner from scratch. You may want to open a browser tab or window to\n",
      "<a href=\"http:MLscalePart3.ipynb\">MLscalePart3.ipynb</a> as a reference. \n",
      "\n",
      "A **Topic model** is a representation of a Bag-Of-Words corpus as several factors or topics. Each topic should represent a theme that recurs in the corpus. Concretely, the output of the topic model will be an (ntopics x nfeatures) matrix we will call <code>tmodel</code>. Each row of that matrix represents a topic, and the elements of that row are word probabilities for the topic (i.e. the rows sum to 1). There is more about topic models <a href=\"http://en.wikipedia.org/wiki/Topic_model\">here on wikipedia</a>.\n",
      "\n",
      "The **element <code>tmodel(i,j)</code> holds the probability that word j belongs to topic i**. Later we will examine the topics directly and try to make sense of them.\n",
      "\n",
      "Now define a mixture class to hold all the options. The learner should use an SFilesDS, an LDA model with IncNorm updater. Add the appropriate code here:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class xopts ...\n",
      "val opts = new xopts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next come the options for the data source. Fill these in using the previous data sources as templates.  \n",
      "\n",
      "You need an SFilesDS (Sparse files datasource), based on files in <code>/data/uci/pubmed_parts/partNN.smat.lz4</code> for NN = 00 to 09. This datasource uses just this one group of files, and each matrix has 141043 rows. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val mdir = \"../data/uci/pubmed_parts/\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we'll create a custom datasource. We'll follow the previous recipe, since the differences are all captured in the options."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val ds = {\n",
      "  implicit val ec = threadPool(4) // make sure there are enough threads (more than the lookahead count)\n",
      "  new SFilesDS(opts)              // the datasource\n",
      "}\n",
      "opts.autoReset = false            // Dont reset the GPU after the training run, so we can use a GPU model for prediction"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next define the main learner class, which is built up from the classes mentioned above: SFilesDS, LDA and IncNorm. For the Learner class, make sure you match the argument positions. \n",
      "\n",
      "LDA is a popular topic model, described <a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">here on wikipedia</a>.\n",
      "\n",
      "We use a fast version of LDA which uses an \"incremental multiplicative update\". That's what the IncNorm updater does. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val nn = "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tuning Options"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Add tuning options for minibatch size (say 100k), number of passes (4) and dimension (<code>dim = 256</code>). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "opts..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You invoke the learner the same way as before. You can change the options above after each run to optimize performance. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn.train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot(nn.results(0,?))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To evaluate the model, we save the model matrix itself, and also load a dictionary of the terms in the corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val tmodel = FMat(nn.modelmat)\n",
      "val dict = Dict(loadSBMat(mdir+\"../pubmed.term.sbmat.gz\"))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dictionary allows us to look up terms by their index, e.g. <code>dict(1000)</code>, by their string represenation <code>dict(\"book\")</code>, and by matrices of these, e.g. <code>dict(ii)</code> where <code>ii</code> is an IMat. Try a few such queries to the dict here:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we evaluate the entropy of each dimension of the model. Recall that the entropy of a discrete probability distribution is $E = -\\sum_{i=1}^n p_i \\ln(p_i)$. The rows of the matrix are the topic probabilities.\n",
      "\n",
      "Compute the entropies for each topic:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val ent = -(tmodel dotr ln(tmodel))\n",
      "ent.t // put them in a horizontal line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Get the mean value (should be positive)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mean(ent)  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Find the smallest and largest entropy topic indices (use maxi2 and mini2). Call them <code>elargest</code> and <code>esmallest</code>."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we'll sort the probabilities within each topic to bring the highest probability terms to the beginning. We sort down (descending order) along dimension 2 (rows) to do this. <code>bestv</code> gets the sorted values and <code>besti</code> gets the sorted indices which are the feature indices."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val (bestp, besti) = sortdown2(tmodel,2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now examine the 100 strongest terms in each topic:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dict(besti(elargest,0->100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dict(besti(esmallest,0->100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do you notice any difference in the coherence of these two topics?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO: Fill in your answer here"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By sorting the entropies, find the 2nd and 3rd smallest entropy topics. Give the top 100 terms in each topic below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "// words for 2nd lowest entropy topic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "// words for 3rd lowest entropy topic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Running more topics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What would you expect to happen to the average topic entropy if you run fewer topics? "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO: answer here"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Change the opts.dim argument above and try it. First note the entropy at dim = 256 below. Then run again with <code>dim=64</code> and put the new value below: "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<table>\n",
      "<tr>\n",
      "<th>dim</th>\n",
      "<th>mean entropy</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>64</td>\n",
      "<td>...</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>256</td>\n",
      "<td>...</td>\n",
      "</tr>\n",
      "</table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You're done! You now have the keys to the fastest machine learning toolkit on earth. Have fun!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}