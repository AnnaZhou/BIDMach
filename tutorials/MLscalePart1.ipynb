{
 "metadata": {
  "name": "",
  "signature": "sha256:31b8021ec295abaaa3b81fbfb7618569588d3263129b6f31e85f4a1f59f14c87"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Machine Learning at Scale, Part I"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For this tutorial, we'll explore Machine Learning at scale. Much of the recent focus on scaling machine learning has been on cluster computing, without much attention to single-machine performance. It turns out that there is a lot of mileage in optimizing single-machine performance for machine learning, especially leveraging inexpensive GPUs. In fact single-node hardware holds the record for most common machine learning tasks. See this <a href=\"https://github.com/BIDData/BIDMach/wiki/Benchmarks\">collection of recent benchmarks</a>. There are several steps to getting high performance on single machines:\n",
      "* \"rooflining\" or optimizing the performance of each kernel against the theoretical limits imposed by memory or the chip's arithmetic units. \n",
      "* Rooflined GPU kernels: there is even more headroom for improvements in machine learning algorithms on graphics processors. \n",
      "* Efficient optimization: Fast optimization (SGD with ADAGRAD or fast coordinate ascent) can add another order of magnitude or more of performance. \n",
      "\n",
      "Each of these can contribute an order-of-magnitude or more of speedup. With the tools you've used so far and modest hardware of your own or in the cloud, you can tackle problems at the frontier of scale for machine learning. First lets initialize BIDMach as usual."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import BIDMat.{CMat,CSMat,DMat,Dict,IDict,Image,FMat,FND,GMat,GIMat,GSMat,HMat,IMat,Mat,SMat,SBMat,SDMat}\n",
      "import BIDMat.MatFunctions._\n",
      "import BIDMat.SciFunctions._\n",
      "import BIDMat.Solvers._\n",
      "import BIDMat.Plotting._\n",
      "import BIDMach.Learner\n",
      "import BIDMach.models.{FM,GLM,KMeans,KMeansw,LDA,LDAgibbs,Model,NMF,SFA}\n",
      "import BIDMach.datasources.{DataSource,MatDS,FilesDS,SFilesDS}\n",
      "import BIDMach.mixins.{CosineSim,Perplexity,Top,L1Regularizer,L2Regularizer}\n",
      "import BIDMach.updaters.{ADAGrad,Batch,BatchNorm,IncMult,IncNorm,Telescoping}\n",
      "import BIDMach.causal.{IPTW}\n",
      "\n",
      "Mat.checkMKL\n",
      "Mat.checkCUDA\n",
      "if (Mat.hasCUDA>0) GPUmem"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This time, you should have a graphics processor, and CUDA version 6.5. If not, something strange is afoot. Check that you started with \"bidmach notebook\" and not \"ipython notebook\". If that's not the problem best to ask for help.\n",
      "\n",
      "The next line is a report on available GPU memory. The first number is the fraction of GPU memory available. It should be close to 1. If not, either you have another java process that is holding memory or someone else on the same machine does. Its fine if other people are using the machine and GPU, but avoid leaving zombie processes of your own around. Check from the shell with <code>ps -ef | grep java</code>. Kill any of your own processes that you dont need with <code>kill -9 process_id</code>."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scaled-up Logistic Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's load the full RCV1 dataset. This dataset is only about 0.5 GB, so we'll load it directly into memory. This is not a big dataset, but has been widely benchmarked and is still challenging for some systems. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val dir = \"../data/\"                                // Assumes you started the notebook from <BIDMach>/tutorials\n",
      "val data = loadSMat(dir+\"rcv1/docs.smat.lz4\")\n",
      "val cats = loadFMat(dir+\"rcv1/cats.fmat.lz4\")\n",
      "val testdata = loadSMat(dir+\"rcv1/testdocs.smat.lz4\")\n",
      "val testcats = loadFMat(dir+\"rcv1/testcats.fmat.lz4\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the types of rcv1data and rcv1cats. They are \"SMat\" and \"FMat\" respectively. These are respectively sparse and dense matrices of float values. Dense matrices are simply arrays with a storage element for each location A(i,j) in a matrix A. Sparse matrices are used for data with many zeros. They contain tuples of (row, column, value) for those elements (sometimes the column index is implicit), rather like a Pandas table. You'll notice they display differently for that reason:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In all the datasets we use, rows will index features (words here) and columns index \"instances\" or input samples, which are documents for this dataset. So <code>rcv1data(?,20)</code> is the $20^{th}$ input sample.\n",
      "\n",
      "Let's construct a learner similar to the one we built in lab, ready for training. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val preds = zeros(testcats.nrows, testcats.ncols)\n",
      "val (rcv, opts, trcv, topts) = GLM.learner(data, cats, testdata, preds, GLM.maxp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can tailor the model's options before lauching it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "opts.batchSize=10000\n",
      "opts.npasses=2\n",
      "opts.autoReset=false"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rcv.train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Understanding Performance Stats"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets examine the columns of the trace above in detail. Taking them in turn they are:\n",
      "* <b>Percent progress meter</b> is the progress over the full dataset. If the algorithm makes multiple passes, each pass is prefaced by a \"pass=X\" title. \n",
      "* <b>Cross-validated log likelihood</b> shows the log likelihood on the most recent held-out minibatch of data. The same minibatches are held out on each pass over the dataset, so that training and test data are not mixed. \n",
      "* <b>gigaflops</b> how many billion arithmetic operations per second the algorithm is completing. Algorithms vary in the number of operations they require, but for a fixed algorithm this is an easy measure to compare performance. There are also \"roofline\" limits for the dominant operations in each algorithm and a particular kind of hardware. The roofline for sparse-dense matrix multiply (the dominant step in logistic regression) is 20-30 gflops for this GPU model, and around 6 gflops for the CPU. You can compare those numbers to see if the algorithm could possibly be optimized any more. \n",
      "* <b>secs</b> is the total elapsed seconds for this training run. \n",
      "* <b>GB</b> the total number of Gigabytes of input data that has been read or re-read during training. \n",
      "* <b>MB/s</b> the data read rate, which is just the quotient of total input bytes by time. This is an important measure because it shows how well the I/O system is doing, and will be the limiting factor for tasks that are not too compute-intensive. Standard disks can manage up to 200 MB/s. SSDs, which are attached to this Amazon instance, can manage up to 500 MB/s. Since BIDMach uses very fast file compression (lz4) you will sometimes see values several times higher than this. That happens because the uncompressed data rate is several times the rate of compressed data coming off the disk. \n",
      "* <b>GPUmem</b> This is the fraction of GPU memory available. It should remain constant because of BIDMach's caching scheme and it serves as a guide to how much space you have, e.g. to increase model size. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets rerun the experiment above (which used the GPU by default), to understand the effect of processor throughput on this calculation. This will take a while. If you want to instead continue with the tutorial, reset the kernel for this page, comment out the \"train\" line below, and restart the page with \"Run All\" under the \"Cell\" menu. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val (rcv2, opts2) = GLM.learner(data, cats, GLM.maxp)\n",
      "opts2.useGPU = false\n",
      "//rcv2.train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see that for this run, performance was limited by the CPU's throughput (gflops) and not by I/O since the I/O system is the same for both runs. For the first run, both Gflops and I/O are close to their respective limits, so its less clear which was the limiting factor."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets evaluate the predictor on the test data, and score it again using category 6 AUC"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trcv.predict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val itest = 6\n",
      "val scores = preds(itest,?)\n",
      "val good = testcats(itest,?)\n",
      "val bad = 1-testcats(itest,?)\n",
      "val rr =roc(scores,good,bad,100)\n",
      "val xaxis = row(0 to 100)*0.01\n",
      "plot(xaxis,rr)\n",
      "val auc = mean(rr)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "val liftx = xaxis(1 to 100)\n",
      "val lift= rr(1 to 100)/liftx\n",
      "plot(liftx, lift)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Performance Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the cell below, make a table of performance for various batch sizes. The columns should be average gflops, overall time, and AUC for cat 6. You should try batch sizes of 10000, 2000 and 1000. You can also see the effect of increasing the number of passes over the dataset. You should find that increasing batchSize, while it reduces accuray a little, can be more than compensated for by increasing the number of iterations. Comment out the cell with the CPU training runs to save time. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> TODO: Fill this table:\n",
      "<table>\n",
      "<tr>\n",
      "<th>Minibatch size</th>\n",
      "<th>Numer of passes</th>\n",
      "<th>Avg. gflops</th>\n",
      "<th>Overall time</th>\n",
      "<th>AUC</th>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>1000</td>\n",
      "<td>2</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>2000</td>\n",
      "<td>2</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>10000</td>\n",
      "<td>2</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>10000</td>\n",
      "<td>20</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "<td>...</td>\n",
      "</tr>\n",
      "</table>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The total work (measured in gflop/s * time) increases with decreasing minibatch size. That's because there are fixed costs associated with the minibatch model updates. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Shutdown this page before continuing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Your job will have used memory on both the CPU and GPU. Its important to free that up before proceeding. Go back to the \"Home\" tab of your IPython browser and click \"shutdown\" on this tutorial. When you're ready go on to part II. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}